{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from darts.datasets import ETTh1Dataset, WineDataset\n",
    "# from darts.models import NLinearModel\n",
    "from darts.metrics.metrics import mae, mse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchmetrics.regression import MeanAbsoluteError, MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_timeline\n",
    "\n",
    "import models.original_models as original_models\n",
    "import models.models_3d_atomics_on_variate_to_concepts as new_models\n",
    "from vasopressor.preprocess_helpers import *\n",
    "from models.helper import *\n",
    "from models.param_initializations import *\n",
    "from models.optimization_strategy import greedy_selection\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "print(device, torch.cuda.current_device())\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_series = WineDataset().load().pd_dataframe()\n",
    "wine_series.plot()\n",
    "len(wine_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def line(x, w=0.3, b=5):\n",
    "    return w*x + b\n",
    "\n",
    "time = np.linspace(0, 500, 1000).reshape(-1,1)\n",
    "\n",
    "trend_series = line(time)\n",
    "trend_series = np.array(trend_series).reshape(-1,1)\n",
    "\n",
    "plt.plot(time, trend_series, marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sine_wave(time, frequency = 1, amplitude = 1, phase = 0):\n",
    "    return amplitude * np.sin(frequency * time + phase)\n",
    "\n",
    "sine_series = sine_wave(time, frequency = 0.05, amplitude = 1.5, phase = np.pi)\n",
    "\n",
    "plt.scatter(time, sine_series)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "components_series = sine_wave(time, frequency = 1, amplitude = 5, phase = np.pi) \\\n",
    "                + sine_wave(time, frequency = 0.1, amplitude = 10, phase = 0) \\\n",
    "                + trend_series\n",
    "\n",
    "plt.scatter(time[:3000], components_series[:3000], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, T, window_stride=1, pred_len=1):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        assert targets.size(0) == data.size(0)\n",
    "        self.T = T # time window\n",
    "        self.window_stride = window_stride\n",
    "        self.pred_len = pred_len\n",
    "        self.N, self.V = data.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(range(0, self.N - self.T - self.pred_len + 1, self.window_stride))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.window_stride\n",
    "        end = start + self.T\n",
    "\n",
    "        X = self.data[start:end]\n",
    "        # if mode == \"S\": # predict only target\n",
    "        y = self.targets[end:end + self.pred_len].flatten()\n",
    "        # elif mode == \"MS\": # predict all variables\n",
    "        #   y = self.data[end:end + self.pred_len, :7].flatten()\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(series, seq_len, window_stride=1, pred_len=1, batch_size = 512):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    train_end = int(len(series) * 0.6)\n",
    "    val_end = int(train_end + len(series) * 0.2)\n",
    "    \n",
    "    train = series[:train_end]\n",
    "    val = series[train_end:val_end]\n",
    "    test = series[val_end:]\n",
    "    \n",
    "    # train, test = series.split_before(0.6)\n",
    "    # val, test = test.split_before(0.5)\n",
    "    \n",
    "    print(\"Train/Val/Test\", len(train), len(val), len(test))\n",
    "    \n",
    "    train = scaler.fit_transform(train)\n",
    "    X_train = pd.DataFrame(train)\n",
    "    y_train = X_train\n",
    "    X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    indicators = torch.isfinite(X_train)\n",
    "    X_train = torch.cat([X_train, indicators], axis=1)\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train, seq_len, window_stride, pred_len)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    val = scaler.transform(val)\n",
    "    X_val = pd.DataFrame(val)\n",
    "    y_val = X_val\n",
    "    X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    indicators = torch.isfinite(X_val)\n",
    "    X_val = torch.cat([X_val, indicators], axis=1)\n",
    "    \n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val, seq_len, window_stride, pred_len)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    test = scaler.transform(test)\n",
    "    X_test = pd.DataFrame(test)\n",
    "    y_test = X_test\n",
    "    X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    indicators = torch.isfinite(X_test)\n",
    "    X_test = torch.cat([X_test, indicators], axis=1)\n",
    "    \n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test, seq_len, window_stride, pred_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "pred_len = 10\n",
    "\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(sine_series, seq_len, pred_len=pred_len)\n",
    "\n",
    "for X,y in train_loader:\n",
    "    print(X.shape, X.requires_grad)\n",
    "    print(y.shape, X.requires_grad)\n",
    "    break\n",
    "\n",
    "i = 0\n",
    "plt.plot(range(seq_len), X[i, :, 0], label=\"X\")\n",
    "plt.plot(range(seq_len), X[i, :, 1], label=\"X IND\")\n",
    "plt.plot(range(seq_len, seq_len + pred_len), y[i], label=\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Batches\", len(train_loader), len(val_loader), len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.plot(train_losses, color=\"black\", label=\"Train\")\n",
    "    plt.plot(val_losses, color=\"green\", label=\"Val\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mae_mse(history, title, dec=\"{:.3g}\"):\n",
    "    xticks = range(len(history[:, 0]))\n",
    "    plt.plot(xticks, history[:, 2], label='MAE')\n",
    "    plt.plot(xticks, history[:, 3], label='MSE')\n",
    "\n",
    "    plt.xlabel('Num Concepts')\n",
    "    plt.ylabel('Criteria')\n",
    "    # plt.ylim(0, 1)\n",
    "    xtick_labels = list(map(int, history[:, 0]))\n",
    "    plt.xticks(xticks, xtick_labels)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    if dec:\n",
    "        for x,_y in zip(xticks, history[:, 2]):\n",
    "            label = dec.format(_y)\n",
    "            plt.annotate(label, # this is the text\n",
    "                        (x,_y), # these are the coordinates to position the label\n",
    "                        textcoords=\"offset points\", # how to position the text\n",
    "                        xytext=(0,-10), # distance from text to points (x,y)\n",
    "                        ha='center') # horizontal alignment can be left, right or center\n",
    "            \n",
    "        for x,_y in zip(xticks, history[:, 3]):\n",
    "            label = dec.format(_y)\n",
    "            plt.annotate(label, # this is the text\n",
    "                        (x,_y), # these are the coordinates to position the label\n",
    "                        textcoords=\"offset points\", # how to position the text\n",
    "                        xytext=(0,-10), # distance from text to points (x,y)\n",
    "                        ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_atomics_concepts_metric(history, title, dec=\"{:.3g}\"):\n",
    "        \n",
    "    df = pd.DataFrame(history, columns=[\"n_atomics\", \"n_concepts\", \"val_loss\", \"mae\", \"mse\"])\n",
    "    mean_atomics = df.groupby(\"n_atomics\").mean()\n",
    "    mean_concepts = df.groupby(\"n_concepts\").mean()\n",
    "\n",
    "    # display(mean_atomics)\n",
    "    plt.plot(mean_atomics.index, mean_atomics[\"mae\"], label='MAE')\n",
    "    plt.plot(mean_atomics.index, mean_atomics[\"mse\"], label='MSE')\n",
    "    plt.xlabel('Num Atomics')\n",
    "    plt.ylabel('Criteria')\n",
    "    plt.title(\"Metric as mean over atomics\")\n",
    "    plt.suptitle(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # display(mean_concepts)\n",
    "    plt.plot(mean_concepts.index, mean_concepts[\"mae\"], label='MAE')\n",
    "    plt.plot(mean_concepts.index, mean_concepts[\"mse\"], label='MSE')\n",
    "    plt.xlabel('Num Concepts')\n",
    "    plt.ylabel('Criteria')\n",
    "    plt.title(\"Metric as mean over concepts\")\n",
    "    plt.suptitle(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_prediction_vs_true(Xb, yb, preds, title):\n",
    "    x = list(range(len(Xb)))\n",
    "    xx = list(range(len(Xb), len(Xb)+len(yb)))\n",
    "    plt.plot(x, Xb, color=\"black\", label=\"True X\")\n",
    "    plt.plot(xx, yb, color=\"blue\", label=\"True y\")\n",
    "    plt.plot(xx, preds, color=\"red\", label=\"Pred y\")\n",
    "    plt.title(title)\n",
    "    plt.yscale(\"linear\")\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim, top_k=''):\n",
    "    model = original_models.CBM(input_dim = input_dim, \n",
    "                            changing_dim = changing_dim, \n",
    "                            seq_len = seq_len,\n",
    "                            num_concepts = n_concepts,\n",
    "                            opt_lr = 3e-3, # 2e-4\n",
    "                            opt_weight_decay = 1e-05,\n",
    "                            l1_lambda=0.001,\n",
    "                            cos_sim_lambda=0.01,\n",
    "                            output_dim = output_dim,\n",
    "                            top_k=top_k,\n",
    "                            task_type=original_models.TaskType.REGRESSION,\n",
    "                            device=device\n",
    "                            )\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def initializeModel_with_atomics(n_atomics, n_concepts, input_dim, changing_dim, seq_len, output_dim, use_summaries_for_atomics, use_indicators, top_k=''):\n",
    "    model = new_models.CBM(input_dim = input_dim, \n",
    "                            changing_dim = changing_dim, \n",
    "                            seq_len = seq_len,\n",
    "                            num_concepts = n_concepts,\n",
    "                            num_atomics = n_atomics,\n",
    "                            use_summaries_for_atomics = use_summaries_for_atomics,\n",
    "                            use_indicators = use_indicators,\n",
    "                            opt_lr = 1e-3, # 2e-4\n",
    "                            opt_weight_decay = 1e-5, # 1e-05,\n",
    "                            l1_lambda=1e-5, # 0.001,\n",
    "                            cos_sim_lambda=1e-5, # 0.01,\n",
    "                            output_dim = output_dim,\n",
    "                            top_k=top_k,\n",
    "                            task_type=new_models.TaskType.REGRESSION,\n",
    "                            )\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "set_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "pred_len = 10\n",
    "n_atomics_list = list(range(2,11,2))\n",
    "n_concepts_list = list(range(2,11,2))\n",
    "changing_dim = 1 #len(series.columns)\n",
    "input_dim = 2 * changing_dim\n",
    "\n",
    "n_atomics = 20\n",
    "n_concepts = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = f\"/workdir/optimal-summaries-public/_models/debug/debugging-L{seq_len}-T{pred_len}/\"\n",
    "# model_path_re = experiment_folder + \"forecasting_c{}_a{}.pt\"\n",
    "model_path_re = experiment_folder + \"forecasting_c{}.pt\"\n",
    "\n",
    "# if not os.path.exists(experiment_folder):\n",
    "#     os.makedirs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_metric = MeanAbsoluteError().to(device)\n",
    "mse_metric = MeanSquaredError().to(device)\n",
    "\n",
    "maes, mses = [], []\n",
    "\n",
    "for _ in range(1):\n",
    "    \n",
    "    train_loader, val_loader, test_loader, scaler = preprocess_data(components_series, seq_len, pred_len=pred_len)\n",
    "    model = initializeModel_with_atomics(n_atomics, n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len, use_summaries_for_atomics=True, use_indicators=False)\n",
    "    # model = initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len)\n",
    "\n",
    "    scheduler = None # torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=model.optimizer, patience=10, factor=0.7, verbose=True)\n",
    "\n",
    "    # model.fit(train_loader, val_loader, None, save_model_path=model_path_re.format(n_concepts, n_atomics), max_epochs=10000, scheduler=scheduler)\n",
    "    model.fit(train_loader, val_loader, None, save_model_path=None, max_epochs=10000, scheduler=scheduler, patience=100)\n",
    "\n",
    "    # display(model)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb)\n",
    "            \n",
    "            mae = mae_metric(preds, yb).item()\n",
    "            mse = mse_metric(preds, yb).item()\n",
    "        mae = mae_metric.compute().item()\n",
    "        mse = mse_metric.compute().item()\n",
    "        mae_metric.reset()\n",
    "        mse_metric.reset()\n",
    "\n",
    "        # print(\"MSE\", round(mse, 5), \"MAE\", round(mae, 5))\n",
    "\n",
    "    maes.append(mae)\n",
    "    mses.append(mse)\n",
    "\n",
    "mae = np.mean(maes)\n",
    "mse = np.mean(mses)\n",
    "print(\"MEAN\", \"MSE\", round(mse, 5), \"MAE\", round(mae, 5))\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "#         Xb, yb = Xb.to(device), yb.to(device)\n",
    "#         preds = model(Xb)\n",
    "#         break\n",
    "\n",
    "\n",
    "#     i = 2\n",
    "#     yb = yb.cpu().numpy()[i].reshape(-1, 1)\n",
    "#     preds = preds.cpu().numpy()[i].reshape(-1, 1)\n",
    "#     Xb = Xb.cpu().numpy()[i, :, 0].reshape(-1, 1)\n",
    "\n",
    "# plot_prediction_vs_true(Xb, yb, preds, title=f\"Redesigned - Predictions with {n_concepts} Concepts, {n_atomics} Atomics\")\n",
    "# display(pd.DataFrame(yb.flatten(), preds.flatten()))\n",
    "\n",
    "\n",
    "# plot_losses(model.train_losses[5:], model.val_losses[5:])\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
    "#         Xb, yb = Xb.to(device), yb.to(device)\n",
    "#         preds = model(Xb)\n",
    "        \n",
    "#         mae = mae_metric(preds, yb).item()\n",
    "#         mse = mse_metric(preds, yb).item()\n",
    "#     mae = mae_metric.compute().item()\n",
    "#     mse = mse_metric.compute().item()\n",
    "#     mae_metric.reset()\n",
    "#     mse_metric.reset()\n",
    "\n",
    "#     print(\"MSE\", round(mse, 5), \"MAE\", round(mae, 5))\n",
    "\n",
    "\n",
    "#     for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
    "#         Xb, yb = Xb.to(device), yb.to(device)\n",
    "#         preds = model(Xb)\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# yb = yb.cpu().numpy()[i].reshape(-1, 1)\n",
    "# preds = preds.cpu().numpy()[i].reshape(-1, 1)\n",
    "# Xb = Xb.cpu().numpy()[i, :, 0].reshape(-1, 1)\n",
    "\n",
    "# plot_prediction_vs_true(Xb, yb, preds, title=f\"Redesigned - Predictions with {n_concepts} Concepts, {n_atomics} Atomics\")\n",
    "# display(pd.DataFrame(yb.flatten(), preds.flatten()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi\n",
    "# relu 20x20\n",
    "MEAN MSE 0.00062 MAE 0.01805\n",
    "\n",
    "# relu 256x256\n",
    "MEAN MSE 0.00161 MAE 0.03197\n",
    "\n",
    "# sigmoid 20x20\n",
    "MEAN MSE 0.00025 MAE 0.01170\n",
    "\n",
    "# sigmoid 256x256\n",
    "MEAN MSE 0.00237 MAE 0.03899\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1ahead\n",
    "# relu 20x20\n",
    "MEAN MSE 0.00174 MAE 0.03148\n",
    "\n",
    "# relu 256x256\n",
    "MEAN MSE 0.00322 MAE 0.04281\n",
    "\n",
    "# sigmoid 20x20\n",
    "MEAN MSE 0.00236 MAE 0.03673\n",
    "\n",
    "# sigmoid 256x256\n",
    "MEAN MSE 0.01041 MAE 0.07715\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prediction vs actual\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(sine_series, seq_len, pred_len=pred_len)\n",
    "\n",
    "mae_metric = MeanAbsoluteError().to(device)\n",
    "mse_metric = MeanSquaredError().to(device)\n",
    "\n",
    "\n",
    "model = initializeModel_with_atomics(n_atomics, n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len, use_summaries_for_atomics=True)\n",
    "# model = initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=model.optimizer, patience=10, factor=0.7, verbose=True)\n",
    "\n",
    "# model.fit(train_loader, val_loader, None, save_model_path=model_path_re.format(n_concepts, n_atomics), max_epochs=10000, scheduler=scheduler)\n",
    "model.fit(train_loader, val_loader, None, save_model_path=model_path_re.format(\"sine_cutoff\"), max_epochs=100000, scheduler=scheduler, patience=100)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        \n",
    "        mae = mae_metric(preds, yb).item()\n",
    "        mse = mse_metric(preds, yb).item()\n",
    "    mae = mae_metric.compute().item()\n",
    "    mse = mse_metric.compute().item()\n",
    "    mae_metric.reset()\n",
    "    mse_metric.reset()\n",
    "\n",
    "    print(\"MSE\", mse, \"MAE\", mae)\n",
    "\n",
    "\n",
    "    for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        break\n",
    "\n",
    "\n",
    "plot_losses(model.train_losses[5:], model.val_losses[5:])\n",
    "\n",
    "\n",
    "i = 20\n",
    "yb = yb.cpu().numpy()[i].reshape(-1, 1)\n",
    "preds = preds.cpu().numpy()[i].reshape(-1, 1)\n",
    "Xb = Xb.cpu().numpy()[i, :, 0].reshape(-1, 1)\n",
    "\n",
    "plot_prediction_vs_true(Xb, yb, preds, title=f\"Redesigned - Predictions with {n_concepts} Concepts, {n_atomics} Atomics\")\n",
    "display(pd.DataFrame(yb.flatten(), preds.flatten()))\n",
    "\n",
    "\n",
    "Xbs = scaler.inverse_transform(Xb)\n",
    "ybs = scaler.inverse_transform(yb)\n",
    "predss = scaler.inverse_transform(preds)\n",
    "\n",
    "plot_prediction_vs_true(Xbs, ybs, predss, title=f\"Redesigned - Predictions with {n_concepts} Concepts, {n_atomics} Atomics\")\n",
    "display(pd.DataFrame(ybs.flatten(), predss.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prediction vs actual\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(components_series, seq_len, pred_len=pred_len)\n",
    "\n",
    "mae_metric = MeanAbsoluteError().to(device)\n",
    "mse_metric = MeanSquaredError().to(device)\n",
    "\n",
    "\n",
    "model = initializeModel_with_atomics(n_atomics, n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len, use_summaries_for_atomics=True)\n",
    "# model = initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=model.optimizer, patience=10, factor=0.7, verbose=True)\n",
    "\n",
    "# model.fit(train_loader, val_loader, None, save_model_path=model_path_re.format(n_concepts, n_atomics), max_epochs=10000, scheduler=scheduler)\n",
    "model.fit(train_loader, val_loader, None, save_model_path=model_path_re.format(\"components_series_cutoff\"), max_epochs=100000, scheduler=scheduler, patience=100)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        \n",
    "        mae = mae_metric(preds, yb).item()\n",
    "        mse = mse_metric(preds, yb).item()\n",
    "    mae = mae_metric.compute().item()\n",
    "    mse = mse_metric.compute().item()\n",
    "    mae_metric.reset()\n",
    "    mse_metric.reset()\n",
    "\n",
    "    print(\"MSE\", mse, \"MAE\", mae)\n",
    "\n",
    "\n",
    "    for batch_idx, (Xb, yb) in enumerate(train_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        break\n",
    "\n",
    "\n",
    "plot_losses(model.train_losses[5:], model.val_losses[5:])\n",
    "\n",
    "\n",
    "i = 20\n",
    "yb = yb.cpu().numpy()[i].reshape(-1, 1)\n",
    "preds = preds.cpu().numpy()[i].reshape(-1, 1)\n",
    "Xb = Xb.cpu().numpy()[i, :, 0].reshape(-1, 1)\n",
    "\n",
    "plot_prediction_vs_true(Xb, yb, preds, title=f\"Redesigned - Predictions with {n_concepts} Concepts, {n_atomics} Atomics\")\n",
    "display(pd.DataFrame(yb.flatten(), preds.flatten()))\n",
    "\n",
    "\n",
    "Xbs = scaler.inverse_transform(Xb)\n",
    "ybs = scaler.inverse_transform(yb)\n",
    "predss = scaler.inverse_transform(preds)\n",
    "\n",
    "plot_prediction_vs_true(Xbs, ybs, predss, title=f\"Redesigned - Predictions with {n_concepts} Concepts, {n_atomics} Atomics\")\n",
    "display(pd.DataFrame(ybs.flatten(), predss.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prediction vs actual\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(components_series, seq_len, pred_len=pred_len)\n",
    "\n",
    "n_concepts = 100\n",
    "n_atomics = 100\n",
    "\n",
    "model = initializeModel_with_atomics(n_atomics, n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len, use_summaries_for_atomics=True)\n",
    "# model = initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len)\n",
    "\n",
    "print(model)\n",
    "# print(model.named_parameters)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape, param.requires_grad, param.device, param.dtype)\n",
    "#     if param.grad is not None:\n",
    "#         print(f\"Parameter: {name}, Gradient: {param}\")\n",
    "\n",
    "\n",
    "model.fit(train_loader, val_loader, None, save_model_path=None, max_epochs=100, patience=100, show_grad=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
