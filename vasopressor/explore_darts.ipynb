{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from darts.datasets import ETTh1Dataset\n",
    "from darts.models import NLinearModel\n",
    "from darts.metrics.metrics import mae, mse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchmetrics.regression import MeanAbsoluteError, MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_timeline\n",
    "\n",
    "# from models import CBM, TaskType\n",
    "from models_redesign import CBM_redesigned, TaskType\n",
    "from preprocess_helpers import *\n",
    "from helper import *\n",
    "from param_initializations import *\n",
    "from optimization_strategy import greedy_selection\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = ETTh1Dataset().load()\n",
    "\n",
    "print(series.start_time())\n",
    "print(series.end_time())\n",
    "\n",
    "series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series, test_series = series.split_before(0.6)\n",
    "val_series, test_series = test_series.split_before(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, T, window_stride=1, pred_len=1):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        assert targets.size(0) == data.size(0)\n",
    "        self.T = T # time window\n",
    "        self.window_stride = window_stride\n",
    "        self.pred_len = pred_len\n",
    "        self.N, self.V = data.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(range(0, self.N - self.T - self.pred_len + 1, self.window_stride))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.window_stride\n",
    "        end = start + self.T\n",
    "\n",
    "        X = self.data[start:end]\n",
    "        y = self.targets[end:end + self.pred_len].squeeze(-1) # only OT\n",
    "        # y = self.data[end:end + self.pred_len, :7].flatten() # all V\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(series, seq_len, window_stride=1, pred_len=1, batch_size = 1024):\n",
    "    scaler = StandardScaler(with_std=False)\n",
    "    \n",
    "    train, test = series.split_before(0.6)\n",
    "    val, test = test.split_before(0.5)\n",
    "    \n",
    "    print(\"Train/Val/Test\", len(train), len(val), len(test))\n",
    "    \n",
    "    train_og = train.pd_dataframe()\n",
    "    train = scaler.fit_transform(train_og)\n",
    "    train = pd.DataFrame(train, columns=train_og.columns)\n",
    "    X_train = train\n",
    "    y_train = train[[\"OT\"]]\n",
    "    X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    indicators = torch.isfinite(X_train)\n",
    "    X_train = torch.cat([X_train, indicators], axis=1)\n",
    "    \n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train, seq_len, window_stride, pred_len)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_og = val.pd_dataframe()\n",
    "    val = scaler.transform(val_og)\n",
    "    val = pd.DataFrame(val, columns=val_og.columns)\n",
    "    X_val = val\n",
    "    y_val = val[[\"OT\"]]\n",
    "    X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    indicators = torch.isfinite(X_val)\n",
    "    X_val = torch.cat([X_val, indicators], axis=1)\n",
    "    \n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val, seq_len, window_stride, pred_len)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    test_og = test.pd_dataframe()\n",
    "    test = scaler.transform(test_og)\n",
    "    test = pd.DataFrame(test, columns=test_og.columns)\n",
    "    X_test = test\n",
    "    y_test = test[[\"OT\"]]\n",
    "    X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "    \n",
    "    indicators = torch.isfinite(X_test)\n",
    "    X_test = torch.cat([X_test, indicators], axis=1)\n",
    "    \n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test, seq_len, window_stride, pred_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test 10451 3484 3485\n",
      "torch.Size([1024, 10, 14])\n",
      "torch.Size([1024, 24])\n",
      "Batches 11 4 4\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(series, seq_len, pred_len=24)\n",
    "\n",
    "for X,y in train_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "\n",
    "print(\"Batches\", len(train_loader), len(val_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.plot(train_losses, color=\"black\", label=\"Train\")\n",
    "    plt.plot(val_losses, color=\"green\", label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim, top_k=''):\n",
    "    logregbottleneck = CBM_redesigned(input_dim = input_dim, \n",
    "                                                changing_dim = changing_dim, \n",
    "                                                seq_len = seq_len,\n",
    "                                                num_concepts = n_concepts,\n",
    "                                                opt_lr = 3e-3, # 2e-4\n",
    "                                                opt_weight_decay = 1e-05,\n",
    "                                                l1_lambda=0.001,\n",
    "                                                cos_sim_lambda=0.01,\n",
    "                                                output_dim = output_dim,\n",
    "                                                top_k=top_k,\n",
    "                                                task_type=TaskType.REGRESSION,\n",
    "                                                )\n",
    "    logregbottleneck = logregbottleneck.to(device)\n",
    "    return logregbottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 96\n",
    "pred_len = 96\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = f\"/workdir/optimal-summaries-public/vasopressor/models/etth1/redesign-multi2single-L{seq_len}-T{pred_len}/\"\n",
    "model_path = experiment_folder + \"forecasting_c{}.pt\"\n",
    "random_seed = 1\n",
    "\n",
    "if not os.path.exists(experiment_folder):\n",
    "    os.makedirs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test 10451 3484 3485\n",
      "n_concepts 2\n",
      "Loaded model from /workdir/optimal-summaries-public/vasopressor/models/etth1/redesign-multi2single-L96-T96/forecasting_c2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "  0%|          | 0/4800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4800 [00:00<51:04,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/4800 [00:00<33:39,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/4800 [00:01<27:33,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/4800 [00:01<25:17,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/4800 [00:01<23:50,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/4800 [00:01<22:54,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/4800 [00:02<22:12,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/4800 [00:02<22:12,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/4800 [00:02<21:17,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4800 [00:02<20:57,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/4800 [00:03<20:44,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/4800 [00:03<20:36,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/4800 [00:03<20:42,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/4800 [00:04<20:32,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/4800 [00:04<20:31,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/4800 [00:04<20:18,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/4800 [00:04<20:20,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/4800 [00:05<20:11,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/4800 [00:05<20:27,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/4800 [00:05<20:34,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/4800 [00:05<21:25,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/4800 [00:06<20:47,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/4800 [00:06<20:39,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/4800 [00:06<20:28,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/4800 [00:06<20:14,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/4800 [00:07<20:28,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/4800 [00:07<20:33,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/4800 [00:07<20:05,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([1024, 96, 14])\n",
      "torch.Size([1024, 96, 98])\n",
      "torch.Size([20, 96, 14])\n",
      "torch.Size([20, 96, 98])\n"
     ]
    }
   ],
   "source": [
    "history_binary = []\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "changing_dim = len(series.columns)\n",
    "input_dim = 2 * changing_dim\n",
    "n_concepts_list = list(range(2,21,2)) + list(np.arange(50,401,50))\n",
    "\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(series, seq_len, pred_len=pred_len)\n",
    "\n",
    "mae_metric = MeanAbsoluteError().to(device)\n",
    "mse_metric = MeanSquaredError().to(device)\n",
    "\n",
    "for n_concepts in n_concepts_list:\n",
    "    print(\"n_concepts\", n_concepts)\n",
    "    \n",
    "    model = initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len)\n",
    "    model.fit(train_loader, val_loader, None, model_path.format(n_concepts), 5000)\n",
    "    \n",
    "    model_shared.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model_shared.forward(Xb)\n",
    "            \n",
    "            mae = mae_metric(preds, yb).item()\n",
    "            mse = mse_metric(preds, yb).item()\n",
    "        mae = mae_metric.compute().item()\n",
    "        mse = mse_metric.compute().item()\n",
    "        mae_metric.reset()\n",
    "        mse_metric.reset()\n",
    "    \n",
    "    history = [n_concepts, round(model_shared.val_losses[-1],2), round(mae,2), round(mse,2)]\n",
    "    display(history)\n",
    "    history_binary.append(np.array(history))\n",
    "    \n",
    "    plot_losses(model_shared.train_losses, model_shared.val_losses)\n",
    "    \n",
    "history_binary = np.array(history_binary)\n",
    "history_binary.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.plot(history_binary[:, 0], history_binary[:, 2], label='MAE')\n",
    "plt.plot(history_binary[:, 0], history_binary[:, 3], label='MSE')\n",
    "\n",
    "plt.xlabel('Num Concepts')\n",
    "plt.ylabel('Criteria')\n",
    "plt.title('Plot of Concepts vs Criteria')\n",
    "plt.xticks(n_concepts_list)\n",
    "plt.xscale('log')\n",
    "\n",
    "for x,_y in zip(history_binary[:, 0], history_binary[:, 2]):\n",
    "    label = \"{:.2f}\".format(_y)\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,_y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "for x,_y in zip(history_binary[:, 0], history_binary[:, 3]):\n",
    "    label = \"{:.2f}\".format(_y)\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,_y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,-10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Prediction vs actual\n",
    "train_loader, val_loader, test_loader, scaler = preprocess_data(series, seq_len, pred_len=pred_len)\n",
    "\n",
    "mae_metric = MeanAbsoluteError().to(device)\n",
    "mse_metric = MeanSquaredError().to(device)\n",
    "n_concepts = 2\n",
    "\n",
    "model_shared = initializeModel(n_concepts, input_dim, changing_dim, seq_len, output_dim=pred_len)\n",
    "model_shared.fit(train_loader, val_loader, None, model_path.format(n_concepts), 10000)\n",
    "\n",
    "model_shared.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (Xb, yb) in enumerate(val_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model_shared.forward(Xb)\n",
    "        \n",
    "        mae = mae_metric(preds, yb).item()\n",
    "        mse = mse_metric(preds, yb).item()\n",
    "        break\n",
    "    mae = mae_metric.compute().item()\n",
    "    mse = mse_metric.compute().item()\n",
    "    mae_metric.reset()\n",
    "    mse_metric.reset()\n",
    "\n",
    "\n",
    "i = 20\n",
    "yb = yb.cpu().numpy()[i]\n",
    "preds = preds.cpu().numpy()[i]\n",
    "\n",
    "print(yb.shape)\n",
    "print(preds.shape)\n",
    "\n",
    "plt.plot(yb, color=\"black\", label=\"True\")\n",
    "plt.plot(preds, color=\"red\", label=\"Pred\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature weights\n",
    "n_concepts = 5\n",
    "\n",
    "model_shared = initializeModel(n_concepts, input_dim, changing_dim, seq_len)\n",
    "model_shared.fit(train_loader, val_loader, None, model_path.format(n_concepts), 1000)\n",
    "\n",
    "for name, param in model_shared.named_parameters():\n",
    "    if \"bottleneck.weight\" in name:\n",
    "        bottleneck_weights = param\n",
    "feature_weights = bottleneck_weights.cpu().detach().numpy()\n",
    "\n",
    "feature_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weight magnitudes\n",
    "for c in range(n_concepts):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    inds = np.argsort(-np.abs(feature_weights[c]))[:100]\n",
    "    ax.bar(np.arange(1,101),np.abs(feature_weights[c])[inds])\n",
    "    ax.set_xlabel(\"Top 100 features\")\n",
    "    ax.set_ylabel(\"abs value of feature coefficient\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 90th percentile of feature weights\n",
    "sum90p = np.sum(np.abs(feature_weights), axis=-1)*0.90\n",
    "sum90p.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top K indizes\n",
    "top_k_inds = []\n",
    "for c in range(n_concepts):\n",
    "    topkinds_conc = []\n",
    "    curr_sum = 0\n",
    "    inds = np.argsort(-np.abs(feature_weights[c])) #desc\n",
    "    sorted_weights = feature_weights[c][inds]\n",
    "    \n",
    "    for ind, weight in zip(inds, sorted_weights):\n",
    "        curr_sum += abs(weight)\n",
    "        if curr_sum <= sum90p[c]:\n",
    "            topkinds_conc.append(ind)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # if selects less than 10, choose 10 best\n",
    "    if len(topkinds_conc) < 10:\n",
    "        topkinds_conc = np.argsort(-np.abs(feature_weights[c]))[:10].tolist()\n",
    "    \n",
    "    top_k_inds.append(topkinds_conc)\n",
    "\n",
    "top_k_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write top k inds to csv\n",
    "filename = experiment_folder + \"top-k/top_k_inds_c{}.csv\".format(n_concepts)\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# writing to csv file \n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # writing the data rows \n",
    "    csvwriter.writerows(top_k_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_aucs, best_auc_inds, best_auc_concepts = greedy_selection(auroc_metric, test_loader, top_k_inds, model_shared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = experiment_folder + \"top-k/bottleneck_r{}_c{}_topkinds.csv\".format(random_seed, n_concepts)\n",
    "\n",
    "# writing to csv file\n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"Best AUC\", \"Best AUC Concept #\", \"Best AUC ind #\"])\n",
    "    # writing the data rows \n",
    "    for row in zip(best_aucs, best_auc_concepts, best_auc_inds):\n",
    "        csvwriter.writerow(list(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = \"/workdir/optimal-summaries-public/vasopressor/models/arabic/multiclass/\"\n",
    "model_path = experiment_folder + \"arabic_c{}.pt\"\n",
    "random_seed = 1\n",
    "\n",
    "if not os.path.exists(experiment_folder):\n",
    "    os.makedirs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_multiclass = []\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "data, y_ohe, num_classes, weights = preprocess_data_multiclass(X, y)\n",
    "train_loader, val_loader, test_loader = initialize_data(1, data, y_ohe, multiclass=True)\n",
    "\n",
    "input_dim = data.shape[2]\n",
    "changing_dim = X[0].shape[0]\n",
    "seq_len = data.shape[1]\n",
    "\n",
    "auroc_metric = AUROC(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "\n",
    "for n_concepts in range(1,16):\n",
    "    print(n_concepts)\n",
    "    \n",
    "    model_shared = initializeModel(n_concepts, input_dim, changing_dim, seq_len, num_classes)\n",
    "    model_shared.fit(train_loader, val_loader, weights, model_path.format(n_concepts), 1000)\n",
    "    \n",
    "    for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        probs = model_shared.forward_probabilities(Xb)\n",
    "        \n",
    "        auc = auroc_metric(probs, yb).item()\n",
    "        acc = accuracy_metric(probs, yb).item()\n",
    "    auc = auroc_metric.compute().item()\n",
    "    acc = accuracy_metric.compute().item()\n",
    "    auroc_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    history = [n_concepts, model_shared.val_losses[-1], auc, acc]\n",
    "    history_multiclass.append(np.array(history))\n",
    "history_multiclass = np.array(history_multiclass)\n",
    "history_multiclass.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.plot(history_multiclass[:, 0], history_multiclass[:, 2], label='AUC')\n",
    "plt.plot(history_multiclass[:, 0], history_multiclass[:, 3], label='ACC')\n",
    "\n",
    "plt.xlabel('Num Concepts')\n",
    "plt.ylabel('Criteria')\n",
    "plt.title('Plot of Concepts vs Criteria')\n",
    "plt.xticks(np.arange(min(history_multiclass[:, 0]), max(history_multiclass[:, 0])+1, 1))\n",
    "\n",
    "for x,_y in zip(history_multiclass[:, 0], history_multiclass[:, 2]):\n",
    "    label = \"{:.2f}\".format(_y)\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,_y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "for x,_y in zip(history_multiclass[:, 0], history_multiclass[:, 3]):\n",
    "    label = \"{:.2f}\".format(_y)\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,_y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,-10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature weights\n",
    "n_concepts = 5\n",
    "\n",
    "model_shared = initializeModel(n_concepts, input_dim, changing_dim, seq_len, num_classes)\n",
    "model_shared.fit(train_loader, val_loader, weights, model_path.format(n_concepts), 1000)\n",
    "\n",
    "for name, param in model_shared.named_parameters():\n",
    "    if \"bottleneck.weight\" in name:\n",
    "        bottleneck_weights = param\n",
    "feature_weights = bottleneck_weights.cpu().detach().numpy()\n",
    "\n",
    "feature_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weight magnitudes\n",
    "for c in range(n_concepts):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    inds = np.argsort(-np.abs(feature_weights[c]))[:100]\n",
    "    ax.bar(np.arange(1,101),np.abs(feature_weights[c])[inds])\n",
    "    ax.set_xlabel(\"Top 100 features\")\n",
    "    ax.set_ylabel(\"abs value of feature coefficient\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 90th percentile of feature weights\n",
    "sum90p = np.sum(np.abs(feature_weights), axis=-1)*0.90\n",
    "sum90p.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top K indizes\n",
    "top_k_inds = []\n",
    "for c in range(n_concepts):\n",
    "    topkinds_conc = []\n",
    "    curr_sum = 0\n",
    "    inds = np.argsort(-np.abs(feature_weights[c])) #desc\n",
    "    sorted_weights = feature_weights[c][inds]\n",
    "    \n",
    "    for ind, weight in zip(inds, sorted_weights):\n",
    "        curr_sum += abs(weight)\n",
    "        if curr_sum <= sum90p[c]:\n",
    "            topkinds_conc.append(ind)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # if selects less than 10, choose 10 best\n",
    "    if len(topkinds_conc) < 10:\n",
    "        topkinds_conc = np.argsort(-np.abs(feature_weights[c]))[:10].tolist()\n",
    "    \n",
    "    top_k_inds.append(topkinds_conc)\n",
    "\n",
    "top_k_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write top k inds to csv\n",
    "filename = experiment_folder + \"top-k/top_k_inds_c{}.csv\".format(n_concepts)\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# writing to csv file \n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # writing the data rows \n",
    "    csvwriter.writerows(top_k_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [i for i in range(1,14)] + [str(i) + \"_ind\" for i in range(1,14)]\n",
    "\n",
    "for c, _list in enumerate(top_k_inds):\n",
    "    for ind in _list:\n",
    "        name, summary = getConcept(data_cols, input_dim, changing_dim, int(ind))\n",
    "        print(f\"Concept {c}: ID {ind}, Feature {name}, Summary {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_results = greedy_selection(auroc_metric, test_loader, top_k_inds, model_shared, track_metrics={\"acc\": accuracy_metric})\n",
    "greedy_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_csv_file = experiment_folder + \"top-k/bottleneck_r{}_c{}_topkinds.csv\".format(random_seed, n_concepts)\n",
    "\n",
    "# writing to csv file\n",
    "with open(top_k_csv_file, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(greedy_results.columns)\n",
    "    # writing the data rows \n",
    "    for row in greedy_results.itertuples(index=False):\n",
    "        csvwriter.writerow(list(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [i for i in range(1,14)] + [str(i) + \"_ind\" for i in range(1,14)]\n",
    "\n",
    "sorted_ = greedy_results.sort_values([\"Concept\", \"ID\"])\n",
    "\n",
    "for row in sorted_.itertuples(index=False):\n",
    "    name, summary = getConcept(data_cols, input_dim, changing_dim, row[1])\n",
    "    print(f\"Concept {row[2]}: ID {row[1]}, Feature {name}, Summary {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(greedy_results[\"Score\"])\n",
    "plt.plot(greedy_results[\"acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_csv_file = \"/workdir/optimal-summaries-public/vasopressor/models/arabic/multiclass/top-k/bottleneck_r1_c6_topkinds.csv\"\n",
    "n_concepts = 6\n",
    "model_shared = initializeModel(n_concepts, input_dim, changing_dim, seq_len, num_classes, top_k=top_k_csv_file)\n",
    "# model.fit(train_loader, val_loader, weights, model_path.format(n_concepts), 1000)\n",
    "\n",
    "model_shared.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        probs = model_shared.forward_probabilities(Xb)\n",
    "        \n",
    "        auc = auroc_metric(probs, yb).item()\n",
    "        acc = accuracy_metric(probs, yb).item()\n",
    "    auc = auroc_metric.compute().item()\n",
    "    acc = accuracy_metric.compute().item()\n",
    "    auroc_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "\n",
    "print(auc)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shared.fit(train_loader, val_loader, weights, save_model_path=\"/workdir/optimal-summaries-public/vasopressor/models/arabic/multiclass/top-k/arabic_c6_finetuned.pt\", epochs=3000)\n",
    "\n",
    "model_shared.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        probs = model_shared.forward_probabilities(Xb)\n",
    "        \n",
    "        auc = auroc_metric(probs, yb)\n",
    "        acc = accuracy_metric(probs, yb)\n",
    "    auc = auroc_metric.compute().item()\n",
    "    acc = accuracy_metric.compute().item()\n",
    "    auroc_metric.reset()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "print(auc)\n",
    "print(acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
